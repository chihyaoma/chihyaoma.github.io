---
layout: post
title:  "Grounded Image Captions without Localization Supervision"
date:   2019-06-03
desc: "Learning to Generate Grounded Image Captions without Localization Supervision"
keywords: "visual captioning"
categories: [Project]
tags:
icon:
---

<meta name="citation_title" content="Learning to Generate Grounded Image Captions without Localization Supervision">
<meta name="citation_author" content="Ma, Chih-Yao">
<meta name="citation_author" content="Kalantidis, Yannis">
<meta name="citation_author" content="AlRegib, Ghassan">
<meta name="citation_author" content="Vajda, Peter">
<meta name="citation_author" content="Rohrbach, Marcus">
<meta name="citation_author" content="Kira, Zsolt">
<meta name="citation_publication_date" content="2019/06/03">
<!-- <meta name="citation_conference_title" content="Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"> -->
<meta name="citation_pdf_url" content="https://arxiv.org/pdf/1906.00283.pdf">

<head>
<style>
.gunimage {
  display: inline-block;
  margin-left: auto;
  margin-right: auto;
  width: 15%;
}
.half {
  width:50%;
  float: left;
}
#images {
  text-align: center;
  width: 100%;
}
div.section_header {
  font-size: x-large;
  color: rgb(30,144,255);
}
</style>
</head>


## **Learning to Generate Grounded Image Captions without Localization Supervision**
[**Chih-Yao Ma**](https://chihyaoma.github.io/), [Yannis Kalantidis](https://www.skamalas.com/), [Ghassan AlRegib](https://ghassanalregib.com/), [Peter Vajda](https://sites.google.com/site/vajdap), [Marcus Rohrbach](https://rohrbach.vision/), [Zsolt Kira](https://www.cc.gatech.edu/~zk15/)<br>
Technical Report, 2019 <br>

[[arXiv]](https://arxiv.org/abs/1906.00283)
[[GitHub]](https://github.com/chihyaoma/cyclical-visual-captioning)

---
<div class="section_header">Abstract</div>
When generating a sentence description for an image, it frequently remains unclear how well the generated caption is grounded in the image or if the model hallucinates based on priors in the dataset and/or the language model. The most common way of relating image regions with words in caption models is through an attention mechanism over the regions that is used as input to predict the next word. The model must therefore learn to predict the attention without knowing the word it should localize. In this work, we propose a novel cyclical training regimen that forces the model to localize each word in the image after the sentence decoder generates it and then reconstruct the sentence from the localized image region(s) to match the ground-truth. The initial decoder and the proposed reconstructor share parameters during training and are learned jointly with the localizer, allowing the model to regularize the attention mechanism. Our proposed framework only requires learning one extra fully-connected layer (the localizer), a layer that can be removed at test time. We show that our model significantly improves grounding accuracy without relying on grounding supervision or introducing extra computation during inference.

---
<div class="section_header">Proposed Concept</div>

<p align="center">
<img src="../../../../static/assets/img/teasers/cyclical.png?raw=true" width="75%">
</p>

---
<div class="section_header">Qualitative Results</div>

We conduct qualitative analysis for comparing the baseline (Unsup.) and the proposed method in the figure below. Each highlighted word has a corresponding image region annotated on the original image. The image regions are selected based on the region with the maximum attention weight. We can see that our proposed method significantly outperformed the baseline (Unsup.) in terms of both the quality of the generated sentence and grounding accuracy. 

<p align="center">
<img src="../../../../static/assets/img/blog/cyclical-vs-baseline.png?raw=true" width="75%">
</p>

In addition, we also show a number of correct and incorrect examples of our proposed method in the figure below.

<p align="center">
<img src="../../../../static/assets/img/blog/cyclical-correct-incorrect.png?raw=true" width="75%">
</p>

---
<div class="section_header">Captioning and Grounding Performance on Flickr30k-Entities</div>
<br>
<span style="vertical-align:middle">
  We evaluate the proposed cyclical training regimen on the Flickr30k dataset for image captioning task.
  To understand how our proposed method performs on captioning as well as visual grounding, we compare with the proposed strong baseline with or without grounding supervision.
  We train the attention mechanism (Attn.) of the baseline method as well as adding the region classification task (Cls.) using the ground-truth grounding annotation. Using the resultant supervised baseline model as the upper bound, our proposed
  method with cyclical training achieves relative 39% and 29% grounding accuracy improvements for
  F1_all and F1_loc respectively, while maintaining the captioning evaluations performances.
</span>
<div>
<img style="display:block; margin-left: auto; margin-right: auto;" src="../../../../static/assets/img/blog/cyclical-captioning-grounding-table.png?raw=true" width="60%">
</div>
<div style="clear: both;"></div>

---
<div class="section_header">Code and Paper</div>
<div id="images">
  <div class="half">
  <a href="https://github.com/chihyaoma/cyclical-visual-captioning">
    <img class="gunimage" alt="idk" src="../../../../static/assets/img/blog/github-icon.png?raw=true">
    <p>GitHub (coming soon)</p>
  <!-- </a> -->
  </div>
  <div class="half">
    <a href="https://arxiv.org/abs/1906.00283">
      <img class="gunimage" alt="idk" src="../../../../static/assets/img/blog/paper-icon.png?raw=true">
      <p>arXiv</p>
    </a>
  </div>
</div>
<div style="clear: both;"></div>

---
<div class="section_header">Citation</div>
If you find this work useful, please cite our paper:
<pre><code>
@article{ma2019learning,
    title={Learning to Generate Grounded Image Captions without Localization Supervision},
    author={Ma, Chih-Yao and Kalantidis, Yannis and AlRegib, Ghassan and Vajda, Peter and Rohrbach, Marcus and Kira, Zsolt},
    journal={arXiv preprint arXiv:1906.00283},
    year={2019},
    url={https://arxiv.org/abs/1906.00283},
}
</code></pre>
