---
layout: post
title:  "Activity Recognition with RNN and Temporal-ConvNet"
date:   2017-03-30
desc: "Activity Recognition with RNN and Temporal-ConvNet"
keywords: "RNN, ConvNet, Action recognition"
categories: [Project]
tags:
icon:
---

<head>
<style>
.gunimage {
  display: inline-block;
  margin-left: auto;
  margin-right: auto;
  width: 15%;
}
.half {
  width:50%;
  float: left;
}
#images {
  text-align: center;
  width: 100%;
}
div.section_header {
  font-size: x-large;
  color: rgb(30,144,255);
}
</style>
</head>

## **TS-LSTM and Temporal-Inception: Exploiting Spatiotemporal Dynamics for Activity Recognition**
[**Chih-Yao Ma\***](https://chihyaoma.github.io/), Min-Hung Chen\*, [Zsolt Kira](https://www.cc.gatech.edu/~zk15/), and [Ghassan AlRegib](https://ghassanalregib.com/)<br>
Signal Processing: Image Communication, 2018<br>
[[arXiv]](http://arxiv.org/abs/1703.10667) [[GitHub]](https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN)<br>
(\* equal contribution)

---
<div class="section_header">Abstract</div>
In this work, we demonstrate a strong baseline two-stream ConvNet using ResNet-101. We use this baseline to thoroughly examine the use of both RNNs and Temporal-ConvNets for extracting spatiotemporal information. Building upon our experimental results, we then propose and investigate two different networks to further integrate spatiotemporal information: 1) temporal segment RNN and 2) Inception-style Temporal-ConvNet.

Our analysis identifies specific limitations for each method that could form the basis of future work. Our experimental results on UCF101 and HMDB51 datasets achieve state-of-the-art performances, 94.1% and 69.0%, respectively, without requiring extensive temporal augmentation.

---
<div class="section_header">How we tackle Activity Recognition problem?</div>
<p align="center">
<img src="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN/blob/master/figures/overview_image.png?raw=true" width="75%">
</p>

---
<div class="section_header">Demo</div>
The GIFs demonstrate the top-3 predictions results of our TS-LSTM and Temporal-Inception methods. The text on the top is the ground truth, three texts are the predictions for each of the method, and the bar right next to the predictions are how confident the model makes predictions.


<p align="center">
<img src="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN/blob/master/figures/demo-1.gif?raw=true" width="30%">
<img src="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN/blob/master/figures/demo-2.gif?raw=true" width="30%">
<img src="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN/blob/master/figures/demo-3.gif?raw=true" width="30%"><br />
<img src="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN/blob/master/figures/demo-4.gif?raw=true" width="30%">
<img src="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN/blob/master/figures/demo-5.gif?raw=true" width="30%">
<img src="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN/blob/master/figures/demo-6.gif?raw=true" width="30%">
</p>

---
<div class="section_header">Code and Paper</div>
<div id="images">
  <div class="half">
  <a href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN">
    <img class="gunimage" alt="idk" src="../../../../static/assets/img/blog/github-icon.png?raw=true">
    <p>GitHub</p>
  </a>
  </div>
  <div class="half">
    <a href="http://arxiv.org/abs/1703.10667">
    <img class="gunimage" alt="idk" src="../../../../static/assets/img/blog/paper-icon.png?raw=true">
    <p>arXiv</p>
    </a>
  </div>
</div>
<div style="clear: both;"></div>

---
<div class="section_header">Citation</div>
If you find this work useful, please cite our paper:
<pre><code>
@article{ma2018ts,
  title={TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition},
  author={Ma, Chih-Yao and Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan},
  journal={Signal Processing: Image Communication},
  year={2018},
  publisher={Elsevier}
}
</code></pre>